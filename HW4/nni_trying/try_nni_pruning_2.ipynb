{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNI in T5\n",
    "Ref : https://nni.readthedocs.io/zh/stable/tutorials/new_pruning_bert_glue.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    DataCollatorForSeq2Seq , \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TALib import TALib\n",
    "ta_lib = TALib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "import nni\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "# from transformers import BertTokenizerFast, DataCollatorWithPadding, BertForSequenceClassification, EvalPrediction\n",
    "# from transformers.trainer import Trainer\n",
    "# from transformers.training_args import TrainingArguments\n",
    "\n",
    "# skip_exec = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_tokenizer():\n",
    "    model =  AutoModelForSeq2SeqLM.from_pretrained(TALib.CHECKPOINT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TALib.TK_ckpt)\n",
    "    return model , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer):\n",
    "    billsum = load_dataset(\"billsum\", split=\"train\")\n",
    "    preprocess_function = TALib.preprocess_function_pass_tokenizer(tokenizer)\n",
    "    tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
    "    \n",
    "    billsum_test = load_dataset(\"billsum\", split=\"test\")\n",
    "    tokenized_billsum_test = billsum_test.map(preprocess_function, batched=True)\n",
    "    \n",
    "    return tokenized_billsum , tokenized_billsum_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_traced_trainer(model, tokenizer ,tokenized_billsum):\n",
    "\n",
    "\n",
    "    compute_metrics = TALib.compute_metrics_pass_tokenizer(tokenizer)\n",
    "    \n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=TALib.CHECKPOINT)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"TA_billsum_model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        weight_decay=0.01,  # Assuming you still want weight decay as it wasn't mentioned to remove\n",
    "        save_total_limit=3,  # Assuming to maintain the save limit as before\n",
    "        num_train_epochs=4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=42,\n",
    "        fp16=True,  # You mentioned \"Native AMP\" for mixed precision training which is generally enabled by setting fp16=True in Transformers\n",
    "        logging_steps=10,  # Assuming to keep the logging frequency as before\n",
    "        predict_with_generate=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer = nni.trace(Seq2SeqTrainer)(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_billsum[\"train\"],\n",
    "        eval_dataset=tokenized_billsum[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_fine_tuning_model():\n",
    "#     model , _ = build_model_and_tokenizer()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# if not skip_exec:\n",
    "#     Path('./output/bert_finetuned').mkdir(exist_ok=True, parents=True)\n",
    "#     build_finetuning_model(task_name, f'./output/bert_finetuned/{task_name}.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.t5 import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.distillation import DynamicLayerwiseDistiller, Adaptive1dLayerwiseDistiller\n",
    "from nni.compression.utils import TransformersEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_distiller(student_model: T5ForConditionalGeneration, teacher_model: T5ForConditionalGeneration,\n",
    "                      student_trainer: Seq2SeqTrainer):\n",
    "    layer_num = len(student_model.encoder.block)\n",
    "    config_list = [{\n",
    "        # 'op_names': [f'bert.encoder.layer.{i}'],\n",
    "        'op_names': [f'encoder.block.{i}'],\n",
    "        # 'link': [f'encoder.block.{j}' for j in range(i, layer_num)],\n",
    "        'link': \"auto\",\n",
    "        'lambda': 0.9,\n",
    "        'apply_method': 'mse',\n",
    "    } for i in range(layer_num)]\n",
    "\n",
    "\n",
    "    evaluator = TransformersEvaluator(student_trainer)\n",
    "\n",
    "    def teacher_predict(batch, teacher_model):\n",
    "        return teacher_model(**batch)\n",
    "\n",
    "    return DynamicLayerwiseDistiller(student_model, config_list, evaluator, teacher_model, teacher_predict, origin_loss_lambda=0.1)\n",
    "\n",
    "\n",
    "def dynamic_distillation(student_model: T5ForConditionalGeneration,\n",
    "                         teacher_model: T5ForConditionalGeneration,\n",
    "                         tokenizer,\n",
    "                         tokenizer_billsum,\n",
    "                         max_steps: int | None,\n",
    "                         max_epochs: int | None):\n",
    "    student_trainer = prepare_traced_trainer(student_model, tokenizer , tokenizer_billsum)\n",
    "\n",
    "    ori_teacher_device = teacher_model.device\n",
    "    training = teacher_model.training\n",
    "    teacher_model.to(student_trainer.args.device).eval()\n",
    "\n",
    "    distiller = dynamic_distiller(student_model, teacher_model, student_trainer)\n",
    "    distiller.compress(max_steps, max_epochs)\n",
    "    distiller.unwrap_model()\n",
    "\n",
    "    teacher_model.to(ori_teacher_device).train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_distiller(student_model: T5ForConditionalGeneration,\n",
    "                    teacher_model: T5ForConditionalGeneration,\n",
    "                    student_trainer: Seq2SeqTrainer):\n",
    "    layer_num = len(student_model.encoder.block)\n",
    "    config_list = [{\n",
    "        'op_names': [f'encoder.block.{i}'],\n",
    "        'lambda': 0.9,\n",
    "        'apply_method': 'mse',\n",
    "    } for i in range(layer_num)]\n",
    "\n",
    "\n",
    "    evaluator = TransformersEvaluator(student_trainer)\n",
    "\n",
    "    def teacher_predict(batch, teacher_model):\n",
    "        return teacher_model(**batch)\n",
    "\n",
    "    return Adaptive1dLayerwiseDistiller(student_model, config_list, evaluator, teacher_model, teacher_predict, origin_loss_lambda=0.1)\n",
    "\n",
    "\n",
    "def adapt_distillation(student_model: T5ForConditionalGeneration, \n",
    "                       teacher_model: T5ForConditionalGeneration,\n",
    "                       tokenizer,\n",
    "                       tokenizer_billsum,\n",
    "                       max_steps: int | None, max_epochs: int | None):\n",
    "    \n",
    "    student_trainer = prepare_traced_trainer(student_model, tokenizer , tokenizer_billsum)\n",
    "\n",
    "    ori_teacher_device = teacher_model.device\n",
    "    training = teacher_model.training\n",
    "    teacher_model.to(student_trainer.args.device).eval()\n",
    "\n",
    "    distiller = adapt_distiller(student_model, teacher_model, student_trainer)\n",
    "    dummy_input = (torch.randint(0, 10000, [8, 128]), torch.randint(0, 2, [8, 128]), torch.randint(0, 2, [8, 128]))\n",
    "    dummy_input = [_.to(student_trainer.args.device) for _ in dummy_input]\n",
    "    distiller.track_forward(*dummy_input)\n",
    "\n",
    "    distiller.compress(max_steps, max_epochs)\n",
    "    distiller.unwrap_model()\n",
    "\n",
    "    teacher_model.to(ori_teacher_device).train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nni.compression.pruning import MovementPruner\n",
    "from nni.compression.speedup import ModelSpeedup\n",
    "from nni.compression.utils.external.external_replacer import TransformersAttentionReplacer\n",
    "\n",
    "\n",
    "def pruning_attn():\n",
    "    Path('./output/bert_finetuned/').mkdir(parents=True, exist_ok=True)\n",
    "    # model = build_finetuning_model(task_name, f'./output/bert_finetuned/{task_name}.bin')\n",
    "    model , tokenizer = build_model_and_tokenizer()\n",
    "    trainer = prepare_traced_trainer(model, tokenizer , None)\n",
    "    evaluator = TransformersEvaluator(trainer)\n",
    "\n",
    "    config_list = [{\n",
    "        'op_types': ['Linear'],\n",
    "        'op_names_re': ['bert\\.encoder\\.layer\\.[0-9]*\\.attention\\.*'],\n",
    "        'sparse_threshold': 0.1,\n",
    "        'granularity': [64, 64]\n",
    "    }]\n",
    "\n",
    "    pruner = MovementPruner(model, config_list, evaluator, warmup_step=9000, cooldown_begin_step=36000, regular_scale=10)\n",
    "    pruner.compress(None, 4)\n",
    "    pruner.unwrap_model()\n",
    "\n",
    "    masks = pruner.get_masks()\n",
    "    Path('./output/pruning/').mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(masks, './output/pruning/attn_masks.pth')\n",
    "    torch.save(model, './output/pruning/attn_masked_model.pth')\n",
    "\n",
    "\n",
    "# if not skip_exec:\n",
    "#     pruning_attn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speedup_attn():\n",
    "    model = torch.load('./output/pruning/attn_masked_model.pth', map_location='cpu')\n",
    "    masks = torch.load('./output/pruning/attn_masks.pth', map_location='cpu')\n",
    "    dummy_input = (torch.randint(0, 10000, [8, 128]), torch.randint(0, 2, [8, 128]), torch.randint(0, 2, [8, 128]))\n",
    "    replacer = TransformersAttentionReplacer(model)\n",
    "    ModelSpeedup(model, dummy_input, masks, customized_replacers=[replacer]).speedup_model()\n",
    "\n",
    "    # finetuning\n",
    "    teacher_model = build_finetuning_model('mnli', f'./output/bert_finetuned/{task_name}.bin')\n",
    "    dynamic_distillation(model, teacher_model, None, 3)\n",
    "    torch.save(model, './output/pruning/attn_pruned_model.pth')\n",
    "\n",
    "\n",
    "if not skip_exec:\n",
    "    speedup_attn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
