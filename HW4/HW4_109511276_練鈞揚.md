# 2024 NYCU Data Science HW4 Model Compression Report

## Written By 109511276 練鈞揚

### GitHub : [Here](https://github.com/KeithLin724)

---

## Result

|State|Model size|RougeScore|Score|Max Gen Length|Open Beam Size(default = 5)|
|---|---|---|---|---|---|
|First time Prune|0.30|0.0000|0.0040|20|False|
|Muti-Step Pruning|0.25|0.1765|0.1352|20|False|
|Muti-Step Pruning-Best|0.26|0.2253|0.1652|20|False|
|Muti-Step Pruning-Best-max-64|0.26|0.3821|0.3085|64|False|
|Muti-Step Pruning-Best-max-64|0.26|0.3821|0.3132|64|True|
|Muti-Step Pruning-Best-max-128|0.26|0.3790|0.3522|128|True|

## Method

In this homework, I used several steps to compress the model. I divided it into four parts for compression with different compression ratios: `T5LayerSelfAttention`, `T5LayerCrossAttention`, `T5LayerFF`, and `lm_head`. Through this assignment, I discovered that `T5LayerCrossAttention` is the critical component that determines the upper bound of score. The idea of splitting the model into multiple parts for compression was inspired by the NNI (AutoML) tutorial.

In the second part of my work, I observed that increasing the model's maximum length beyond 20 results in a better score than the default. I experimented with maximum lengths of 64 and 128, with 64 proving to be the optimal choice for this homework.

Below, I provide a sample of the code demonstrating how I compressed the model:

1. Choose the pruning ratio

    ```python
    self_attention_prune_amount = 0.9
    cross_attention_prune_amount = 0.4
    ff_prune_amount = 0.7
    lm_head_amount = 0.8
    ```

2. Choose the pruning ratio

    ```python
    fine_tune_model_epochs = 15
    self_attention_epochs = 15
    cross_attention_epochs = 20
    ff_prune_epochs = 15
    lm_head_epochs = 30
    ```

3. Split the part

    ```python
    parameters_to_prune = {"self_attention" : [] , "cross_attention":[] , "ffn":[] , "lm_head":[]}
    for name, module in model.named_modules():
        # print(name , type(module))
        if isinstance(module ,T5LayerSelfAttention ):
            # print("SelfAttention " , module)
            
            for name_2 , item in module.named_modules():
                if isinstance(item , torch.nn.Linear):
                    parameters_to_prune["self_attention"].append((item , "weight"))
                    
        if isinstance(module ,T5LayerCrossAttention ):
            # print("CrossAttention " , module)
            
            for name_2 , item in module.named_modules():
                if isinstance(item , torch.nn.Linear):
                    parameters_to_prune["cross_attention"].append((item , "weight"))
                    
        if isinstance(module ,T5LayerFF ):
            # print("FFN " , module)
            
            for name_2 , item in module.named_modules():
                if isinstance(item , torch.nn.Linear):
                    parameters_to_prune["ffn"].append((item , "weight"))
                    
        if isinstance(module ,torch.nn.Linear ) and name == "lm_head":
            parameters_to_prune["lm_head"].append((module , "weight"))
    ```

4. prune the model

  ```python
  trainer.set_train_epochs(fine_tune_model_epochs)

  prune.global_unstructured(
      parameters_to_prune["self_attention"],
      pruning_method=prune.L1Unstructured,
      amount=self_attention_prune_amount,
  )

  trainer.set_train_epochs(self_attention_epochs)
  trainer.train()

  prune.global_unstructured(
      parameters_to_prune["cross_attention"],
      pruning_method=prune.L1Unstructured,
      amount=cross_attention_prune_amount,
  )

  trainer.set_train_epochs(cross_attention_epochs)
  trainer.train()

  prune.global_unstructured(
      parameters_to_prune["ffn"],
      pruning_method=prune.L1Unstructured,
      amount=ff_prune_amount,
  )

  trainer.set_train_epochs(ff_prune_epochs)
  trainer.train()

  prune.global_unstructured(
      parameters_to_prune["lm_head"],
      pruning_method=prune.L1Unstructured,
      amount=lm_head_amount,
  )

  trainer.set_train_epochs(lm_head_epochs)
  trainer.train()
  ```

This sample code illustrates how I applied compression techniques to different components of the T5 model to optimize performance while reducing complexity.

## Model Structure of T5 small

```txt
T5ForConditionalGeneration(
  (shared): Embedding(32128, 512)
  (encoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-5): 5 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): T5Stack(
    (embed_tokens): Embedding(32128, 512)
    (block): ModuleList(
      (0): T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
              (relative_attention_bias): Embedding(32, 8)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1-5): 5 x T5Block(
        (layer): ModuleList(
          (0): T5LayerSelfAttention(
            (SelfAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): T5LayerCrossAttention(
            (EncDecAttention): T5Attention(
              (q): Linear(in_features=512, out_features=512, bias=False)
              (k): Linear(in_features=512, out_features=512, bias=False)
              (v): Linear(in_features=512, out_features=512, bias=False)
              (o): Linear(in_features=512, out_features=512, bias=False)
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): T5LayerFF(
            (DenseReluDense): T5DenseActDense(
              (wi): Linear(in_features=512, out_features=2048, bias=False)
              (wo): Linear(in_features=2048, out_features=512, bias=False)
              (dropout): Dropout(p=0.1, inplace=False)
              (act): ReLU()
            )
            (layer_norm): T5LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (final_layer_norm): T5LayerNorm()
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (lm_head): Linear(in_features=512, out_features=32128, bias=False)
)
```

---

## References

### Pytorch official pruning tutorial

Torch Pruning : [Pruning](https://pytorch.org/tutorials/intermediate/pruning_tutorial.html)

### NNI

Video : [Here](https://www.youtube.com/watch?v=wKh51Jnr0a8)

Github : [Here](https://github.com/microsoft/nni/)

Web : [Here](https://nni.readthedocs.io/zh/stable/index.html)

Pruning

1. Masking simulation
2. Speedup
3. Fine-tuning

Step

1. Weight matrix
2. ((from NNI Pruner)mask (0, 1) , weight matrix)
3. masked weights
4. NNI Speed up

nni in transformer : [Here](https://www.infoq.cn/article/6mA1gDVFWU1oj1ZdQyD2)

nni in transformer example : [Here](https://nni.readthedocs.io/en/stable/tutorials/new_pruning_bert_glue.html)

nni config list : [Here](https://nni.readthedocs.io/zh/stable/compression/config_list.html)
