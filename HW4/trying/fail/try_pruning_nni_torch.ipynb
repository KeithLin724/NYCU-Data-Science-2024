{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer , T5ForConditionalGeneration,Seq2SeqTrainer,Seq2SeqTrainingArguments\n",
    "from transformers.models.t5.modeling_t5 import T5LayerSelfAttention , T5Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TALib import TALib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TALib.TK_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(TALib.CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_pruning as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"translate English to French: Hello, how are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "decoder_input_ids = tokenizer(\"Ce message est une traduction :\", return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = {\"input_ids\":input_ids, \"decoder_input_ids\":decoder_input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = tp.importance.MagnitudeImportance(p=2, group_reduction=\"mean\")\n",
    "# base_macs, base_params = tp.utils.count_ops_and_params(model, dummy_input)\n",
    "num_heads = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.modules():\n",
    "    if isinstance(m, T5Attention):\n",
    "        \n",
    "        num_heads[m.q] = m.n_heads\n",
    "        num_heads[m.k] = m.n_heads\n",
    "        num_heads[m.v] = m.n_heads\n",
    "\n",
    "        \n",
    "        \n",
    "        # print(m)\n",
    "        # num_heads[m.query] = m.num_attention_heads\n",
    "        # num_heads[m.key] = m.num_attention_heads\n",
    "        # num_heads[m.value] = m.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = tp.pruner.MetaPruner(\n",
    "    model, \n",
    "    dummy_input, \n",
    "    global_pruning=False, # If False, a uniform pruning ratio will be assigned to different layers.\n",
    "    importance=imp, # importance criterion for parameter selection\n",
    "    iterative_steps=1, # the number of iterations to achieve target pruning ratio\n",
    "    pruning_ratio=0.7,\n",
    "    num_heads=num_heads,\n",
    "    prune_head_dims=False,\n",
    "    prune_num_heads=True,\n",
    "    head_pruning_ratio=0.5,\n",
    "    # output_transform=lambda out: out.pooler_output.sum(),\n",
    "    # ignored_layers=[model.],\n",
    ")\n",
    "\n",
    "for g in pruner.step(interactive=True):\n",
    "    #print(g)\n",
    "    g.prune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TALib.show_param_ratio(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function = TALib.preprocess_function_pass_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=TALib.CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"TA_billsum_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,  # Assuming you still want weight decay as it wasn't mentioned to remove\n",
    "    save_total_limit=3,  # Assuming to maintain the save limit as before\n",
    "    num_train_epochs=4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    fp16=True,  # You mentioned \"Native AMP\" for mixed precision training which is generally enabled by setting fp16=True in Transformers\n",
    "    logging_steps=10,  # Assuming to keep the logging frequency as before\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "compute_metrics = TALib.compute_metrics_pass_tokenizer(tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum_test = load_dataset(\"billsum\", split=\"test\")\n",
    "tokenized_billsum_test = billsum_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(tokenized_billsum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.predict(tokenized_billsum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_prediction = tokenizer.batch_decode(results[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TALib.dump_to_kaggle_format(decoded_prediction , 'pruned_model_0.3_real_torch_pruning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = TALib.run_score(predict=decoded_prediction,label=billsum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output/pruning\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,  # Assuming you still want weight decay as it wasn't mentioned to remove\n",
    "    save_total_limit=3,  # Assuming to maintain the save limit as before\n",
    "    num_train_epochs=1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    fp16=True,  # You mentioned \"Native AMP\" for mixed precision training which is generally enabled by setting fp16=True in Transformers\n",
    "    logging_steps=10,  # Assuming to keep the logging frequency as before\n",
    "    predict_with_generate=True,\n",
    "\n",
    ")\n",
    "\n",
    "compute_metrics = TALib.compute_metrics_pass_tokenizer(tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TALib.save_model(model , \"output/try_pruning_0.3_torch_pruning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# 加载预训练的 T5 模型和分词器\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 创建一个示例输入\n",
    "input_text = \"Translate English to French: How are you?\"\n",
    "\n",
    "# 使用分词器对输入进行编码\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 剪枝前，查看模型的参数量\n",
    "print(f\"模型参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# 对模型中的某些层进行剪枝\n",
    "# 例如，剪枝 encoder 中第一个 self-attention 层的权重\n",
    "# 对 T5 模型中的自注意力层进行剪枝\n",
    "prune.ln_structured(model.encoder.block[0].layer[0].SelfAttention.q,\n",
    "                    name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "prune.ln_structured(model.encoder.block[0].layer[0].SelfAttention.k,\n",
    "                    name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "prune.ln_structured(model.encoder.block[0].layer[0].SelfAttention.v,\n",
    "                    name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "prune.ln_structured(model.encoder.block[0].layer[0].SelfAttention.o,\n",
    "                    name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "\n",
    "# 剪枝后，查看模型的参数量\n",
    "print(f\"剪枝后模型参数量：{sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TALib.show_param_ratio(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
