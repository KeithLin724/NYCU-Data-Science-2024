{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e38128d-4a3e-4685-992a-effad72ecc1d",
   "metadata": {},
   "source": [
    "# Data Science HW4 Tutorial\n",
    "## Reference: https://huggingface.co/docs/transformers/tasks/summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1d2033-adff-42b0-afd9-9de5a4f56c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # change device number if there exists more than one gpu on your platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc3c4f0-a5f1-435d-9792-b4122adffe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets  import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871901b-dacf-4214-b681-422692082491",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a17bb97-639a-48f0-850e-37fb834fb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5small_TextSummarization/\" # released full model path\n",
    "TK_ckpt = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TK_ckpt)  # use tokeniozer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811efa96-affc-44f8-b161-1f153743472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3671d0a3-1fc8-4de4-b256-eaad6941b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22dc7826-e1e5-4e1d-b092-9f8ac7b7d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db040045-043b-404f-b030-c9a6debd8b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76e55b1b-aa2b-4119-b613-80e49e8768e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "# load full model \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386eb9e1-6d09-431c-a098-c0ed025b7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c972f9ce-68cf-4f0e-9468-6af941689873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6a31131f7a4863a472365299135d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285932d7ece74d3f9d78f7a77c9904b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3790 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)\n",
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9e7f9f-95f4-4ca6-b167-5e13e835a2c3",
   "metadata": {},
   "source": [
    "## TA's trainer for fine-tune T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbd5461-bd62-4818-8d3f-69b953435685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"TA_billsum_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,  # Assuming you still want weight decay as it wasn't mentioned to remove\n",
    "    save_total_limit=3,  # Assuming to maintain the save limit as before\n",
    "    num_train_epochs=4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=42,\n",
    "    fp16=True,  # You mentioned \"Native AMP\" for mixed precision training which is generally enabled by setting fp16=True in Transformers\n",
    "    logging_steps=10,  # Assuming to keep the logging frequency as before\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f0686-dd7f-48c3-a12f-5f87070cab23",
   "metadata": {},
   "source": [
    "## Ratio of non-zero parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89716f50-5735-4f7f-8c6d-9712717efd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_param_ratio(model):\n",
    "    num_param = 0\n",
    "    for param in model.parameters():\n",
    "        num_param += param.numel()\n",
    "    num_mask = 0\n",
    "    for name, param in model.named_buffers():\n",
    "        if \"mask\" in name:\n",
    "            num_mask += (param == 0).sum()\n",
    "    print((num_param - num_mask) / num_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3427cb-61ad-4c91-9972-69f7a1b5937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "show_param_ratio(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6ecb7-bc87-4233-99b0-bb45db24c9a4",
   "metadata": {},
   "source": [
    "## Prediction Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d99afdcf-c099-4eef-95cb-8870d5bfde2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6b7e144a3b4109ab67855ce23f1e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "billsum_test = load_dataset(\"billsum\", split=\"test\")\n",
    "tokenized_billsum_test = billsum_test.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b9fb6ef-aaf4-4b06-94c5-13032deec033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/erictseng/virtual_env_dir/DS_TA/lib/python3.8/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.4963321685791016,\n",
       " 'eval_rouge1': 0.2411,\n",
       " 'eval_rouge2': 0.196,\n",
       " 'eval_rougeL': 0.2334,\n",
       " 'eval_rougeLsum': 0.2333,\n",
       " 'eval_gen_len': 18.9997,\n",
       " 'eval_runtime': 259.0677,\n",
       " 'eval_samples_per_second': 12.618,\n",
       " 'eval_steps_per_second': 6.311}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_billsum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b83c8b8-bb2e-4f6c-8d67-f53d625fe1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/erictseng/virtual_env_dir/DS_TA/lib/python3.8/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = trainer.predict(tokenized_billsum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbfd5821-7571-47b7-a3f3-adae1a123831",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_prediction = tokenizer.batch_decode(results[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "047ff645-68ef-41be-b64d-dff82b0d661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8c4da8f-16d0-4ec3-9018-44334504b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                            Predict\n",
      "0        0  Amends the Water Resources Development Act of ...\n",
      "1        1  Federal Forage Fee Act of 1993 - Requires all ...\n",
      "2        2  Merchant Marine of World War II Congressional ...\n",
      "3        3  Small Business Tax Modernization Act of 2004 -...\n",
      "4        4  Fair Access to Investment Research Act of 2016...\n",
      "...    ...                                                ...\n",
      "3264  3264  Public Servant Priority Placement Act of 1995 ...\n",
      "3265  3265  Sportmanship in Hunting Act of 2008 - Amends t...\n",
      "3266  3266  Helping College Students Cross the Finish Line...\n",
      "3267  3267  Texas National Forests Improvement Act of 2000...\n",
      "3268  3268  Federal Power Asset Privatization Act of 1995 ...\n",
      "\n",
      "[3269 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(columns=['ID','Predict'])\n",
    "\n",
    "for i, prediction in enumerate(decoded_prediction):\n",
    "    # Escape quotes by replacing \",\" with \".\"\n",
    "    summary_escaped = prediction.replace(',', '.')\n",
    "    \n",
    "    # Create a new row DataFrame and append it\n",
    "    new_row = pd.DataFrame({'ID': [i], 'Predict': [summary_escaped]})\n",
    "    df_results = pd.concat([df_results, new_row], ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "603fcc88-75b8-4f70-8c03-fe6d7a0dc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to escape double quotes and handle newlines\n",
    "def escape_special_characters(text):\n",
    "    return text.replace('\"', '\"\"').replace('\\n', ' ')\n",
    "\n",
    "# Apply escaping to the 'Summary' column\n",
    "df_results['Predict'] = df_results['Predict'].apply(escape_special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f427e-58eb-4edf-b6dc-dbe95c136b2c",
   "metadata": {},
   "source": [
    "### Dump Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b574eded-d30d-4a4e-971e-94f97d7a6a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.to_csv('full_model_sample_submission.csv', index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a28d7-4b4f-48d1-8002-a44e15a52de2",
   "metadata": {},
   "source": [
    "### Calculating ROUGE-Lsum with build-in Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ae31199-f959-4cf0-94e0-0446a5c1b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lcs(X, Y):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the longest common subsequence of sequences X and Y.\n",
    "    \"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    L = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i - 1] == Y[j - 1]:\n",
    "                L[i][j] = L[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "\n",
    "    return L[m][n]\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Computes the ROUGE-Lsum score based on the longest common subsequence summed over all sentences in the summaries.\n",
    "    \n",
    "    Args:\n",
    "    solution (pd.DataFrame): The DataFrame containing the correct summaries.\n",
    "    submission (pd.DataFrame): The DataFrame containing participant's predicted summaries.\n",
    "    row_id_column_name (str): The column name for the row ID in both DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    float: The mean ROUGE-Lsum score across all predictions.\n",
    "    \"\"\"\n",
    "    # Ensure indices for proper alignment\n",
    "    solution.set_index(row_id_column_name, inplace=True)\n",
    "    submission.set_index(row_id_column_name, inplace=True)\n",
    "\n",
    "    total_score = 0\n",
    "\n",
    "    for idx in solution.index:\n",
    "        if idx not in submission.index:\n",
    "            raise ParticipantVisibleError(f\"Missing prediction for ID {idx}.\")\n",
    "\n",
    "        ref_summary = solution.loc[idx, 'Label']\n",
    "        pred_summary = submission.loc[idx, 'Predict']\n",
    "\n",
    "        # Tokenize sentences\n",
    "        ref_sentences = ref_summary.split('.')\n",
    "        pred_sentences = pred_summary.split('.')\n",
    "\n",
    "        # Calculate LCS for each sentence pair\n",
    "        lcs_sum = 0\n",
    "        for ref_sent in ref_sentences:\n",
    "            ref_tokens = ref_sent.strip().lower().split()\n",
    "            best_lcs = 0\n",
    "            for pred_sent in pred_sentences:\n",
    "                pred_tokens = pred_sent.strip().lower().split()\n",
    "                lcs_length = calculate_lcs(ref_tokens, pred_tokens)\n",
    "                best_lcs = max(best_lcs, lcs_length)\n",
    "            lcs_sum += best_lcs\n",
    "\n",
    "        # Calculate ROUGE-L for the current pair of summaries\n",
    "        ref_length = sum(len(sent.strip().split()) for sent in ref_sentences)\n",
    "        if ref_length > 0:\n",
    "            rouge_l = lcs_sum / ref_length\n",
    "        else:\n",
    "            rouge_l = 0\n",
    "        total_score += rouge_l\n",
    "\n",
    "    # Compute the average ROUGE-L score across all submissions\n",
    "    mean_rouge_lsum = total_score / len(solution)\n",
    "\n",
    "    return mean_rouge_lsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a9c1f39-e16e-483b-9b63-e87ce6ab32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                              Label\n",
      "0        0  Amends the Water Resources Development Act of ...\n",
      "1        1  Federal Forage Fee Act of 1993 - Subjects graz...\n",
      "2        2  .  Merchant Marine of World War II Congression...\n",
      "3        3  Small Business Modernization Act of 2004 - Ame...\n",
      "4        4  Fair Access to Investment Research Act of 2016...\n",
      "...    ...                                                ...\n",
      "3264  3264  Public Servant Priority Placement Act of 1995 ...\n",
      "3265  3265  Sportsmanship in Hunting Act of 2008 - Amends ...\n",
      "3266  3266  Helping College Students Cross the Finish Line...\n",
      "3267  3267  Makes proceeds from such conveyances available...\n",
      "3268  3268  Federal Power Asset Privatization Act of 1995 ...\n",
      "\n",
      "[3269 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df_label = pd.DataFrame(columns=['ID','Label'])\n",
    "\n",
    "for i, label in enumerate(billsum_test):\n",
    "    # Escape quotes by replacing \",\" with \".\"\n",
    "    label_escaped = label['summary'].replace(',', '.')\n",
    "    \n",
    "    # Create a new row DataFrame and append it\n",
    "    new_row = pd.DataFrame({'ID': [i], 'Label': [label_escaped]})\n",
    "    df_label = pd.concat([df_label, new_row], ignore_index=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5a94ba-23af-4849-a105-798182fe9e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16893694028616194"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(df_label, df_results, 'ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66712a3-10b3-4dc9-b03a-5a0531a39f02",
   "metadata": {},
   "source": [
    "## Sample code to do pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9debf1-c8be-42e3-9aa7-c7680da4c2f0",
   "metadata": {},
   "source": [
    "torch.nn tutorial: https://pytorch.org/tutorials/intermediate/pruning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b2e0e-8b26-4590-b712-0e88e3f7e0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = []\n",
    "for _, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        parameters_to_prune.append((module, \"weight\"))\n",
    "torch.nn.utils.prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=    ,\n",
    "    amount= prune,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09417f53-956e-4437-9d62-db5e12550f5d",
   "metadata": {},
   "source": [
    "### Check pruned non-zero ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65671a0-454e-4154-8abe-682bfd3ab1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_param_ratio(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
