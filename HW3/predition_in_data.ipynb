{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/python/NYCU-Data-Science-2024/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from rich import print\n",
    "from datasets import load_dataset , load_metric\n",
    "from transformers import AutoTokenizer , BatchEncoding, BartForConditionalGeneration, BartTokenizer\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headline', 'body'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\" , data_files=\"./data/train.json\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['headline', 'body'],\n",
       "        num_rows: 99900\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['headline', 'body'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test = dataset[\"train\"].train_test_split(test_size=100)\n",
    "sample_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/code/python/NYCU-Data-Science-2024/HW3'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"facebook/bart-base\"\n",
    "model_id = \"model/model/data_science_hw3_model_facebook-bart-large/checkpoint-6300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizer(name_or_path='model/model/data_science_hw3_model_facebook-bart-large/checkpoint-6300', vocab_size=50265, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Model is running on cu<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">da:0</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Model is running on cu\u001b[1;92mda:0\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 检查模型参数的设备\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 打印设备信息\n",
    "print(f\"Model is running on {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# max_input_length = 1024\n",
    "# max_target_length = 64\n",
    "# # 定义数据集\n",
    "# class YourDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, dataset):\n",
    "#         self.dataset = dataset\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # 处理数据集，返回所需的数据\n",
    "#         test_item = self.dataset[idx]\n",
    "#         label = tokenizer(test_item[\"headline\"], max_length=max_target_length, truncation=True, padding=True, return_tensors=\"pt\").to(device) \n",
    "#         pre_text_input = tokenizer(test_item[\"body\"], max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "#         return  label, pre_text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check: 100%|██████████| 100/100 [02:35<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "label_tensor_arr = []\n",
    "pre_tensor_arr = []\n",
    "max_input_length = 1024\n",
    "max_target_length = 64\n",
    "\n",
    "# 将模型移到相同的设备上\n",
    "# model.to(device)\n",
    "\n",
    "for test_item in tqdm(sample_test[\"test\"], desc=\"check\"):\n",
    "    # 将输入张量移动到设备\n",
    "    label = tokenizer(test_item[\"headline\"], max_length=max_target_length, truncation=True, padding=True, return_tensors=\"pt\").to(device) \n",
    "    pre_text_input = tokenizer(test_item[\"body\"], max_length=max_input_length, truncation=True, padding=True, return_tensors=\"pt\").to(device) \n",
    "    \n",
    "    # 生成预测\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **pre_text_input,\n",
    "            num_beams=10,\n",
    "            early_stopping=True,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            no_repeat_ngram_size=2,\n",
    "        )\n",
    "\n",
    "    label_tensor_arr.append(test_item[\"headline\"])\n",
    "    pre_tensor_arr.append(outputs[0].cpu())\n",
    "\n",
    "# for (str_input, labels, pre_text_inputs) in tqdm(data_loader, desc=\"check\"):\n",
    "#     # 生成预测\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(\n",
    "#             **pre_text_inputs,\n",
    "#             num_beams=5,\n",
    "#             early_stopping=True,\n",
    "#             no_repeat_ngram_size=2,\n",
    "#         )\n",
    "\n",
    "#     label_tensor_arr.extend(str_input)\n",
    "#     pre_tensor_arr.extend(outputs[0].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rouge = evaluate.load(\"rouge\",rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "metric_bert_score = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(label_tensor_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred) -> dict:\n",
    "    predictions , labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions , skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_labels = labels\n",
    "\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
    "                      for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) \n",
    "                      for label in decoded_labels]\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    result = metric_rouge.compute(predictions=decoded_preds, references=decoded_labels,\n",
    "                            use_stemmer=True)\n",
    "\n",
    "    result_bert_score = metric_bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\") # model_type=\"distilbert-base-uncased\",\n",
    "    \n",
    "    # Extract ROUGE f1 scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # add the bert score f1 mean\n",
    "    result[\"BERTScore f1 mean\"] = np.mean(result_bert_score[\"f1\"]) * 100\n",
    "    \n",
    "    # Add mean generated length to metrics\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
    "                      for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "ans = compute_metrics((pre_tensor_arr , label_tensor_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 50.9011,\n",
       " 'rouge2': 34.8813,\n",
       " 'rougeL': 47.4495,\n",
       " 'rougeLsum': 47.8271,\n",
       " 'BERTScore f1 mean': 91.106,\n",
       " 'gen_len': 17.35}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans\n",
    "\n",
    "# # 编码输入文本\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device) # .input_ids\n",
    "# print(input_ids)\n",
    "# # 生成文本\n",
    "# outputs = model.generate(\n",
    "#     **input_ids,\n",
    "#     num_beams=5,\n",
    "#     early_stopping=True,\n",
    "#     no_repeat_ngram_size=2,\n",
    "# )\n",
    "\n",
    "# # 解码生成的文本\n",
    "# decoded_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " n = 10 {'rouge1': 50.3037,\n",
    " 'rouge2': 34.7642,\n",
    " 'rougeL': 46.8491,\n",
    " 'rougeLsum': 47.1943,\n",
    " 'BERTScore f1 mean': 91.1044,\n",
    " 'gen_len': 17.41}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(decoded_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
